# -*- coding: utf-8 -*-
"""Cross_camera_Player.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eG-4--IXCsfieN1KzbA3Ld9yhuMFDi1o
"""

from google.colab import drive
drive.mount('/content/drive')

pip install ultralytics torch torchvision torchreid opencv-python deep_sort_realtime

from ultralytics import YOLO
import cv2

def run_yolo_inference(video_path, model_path='/content/drive/MyDrive/best.pt'):
    model = YOLO(model_path)
    cap = cv2.VideoCapture(video_path)
    detections = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        results = model(frame)[0]
        frame_detections = []
        for box in results.boxes:
            cls = int(box.cls[0])
            if cls in [0, 1, 2]:  # player types
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                frame_detections.append((x1, y1, x2, y2, cls))
        detections.append(frame_detections)
    cap.release()
    return detections

from deep_sort_realtime.deepsort_tracker import DeepSort

def track_players(video_path, detections):
    cap = cv2.VideoCapture(video_path)
    tracker = DeepSort(max_age=30)
    tracks = []

    frame_idx = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret or frame_idx >= len(detections):
            break
        frame_dets = detections[frame_idx]
        input_dets = [((x1, y1, x2 - x1, y2 - y1), 0.99, cls) for (x1, y1, x2, y2, cls) in frame_dets]
        track_outputs = tracker.update_tracks(input_dets, frame=frame)
        frame_tracks = []
        for t in track_outputs:
            if not t.is_confirmed():
                continue
            track_id = t.track_id
            ltrb = t.to_ltrb()
            frame_tracks.append((track_id, *ltrb))
        tracks.append(frame_tracks)
        frame_idx += 1
    cap.release()
    return tracks

import torchreid
import torch
import numpy as np
import torchvision.transforms as T

torchreid.models.show_avai_models()
model = torchreid.models.build_model('osnet_x1_0', num_classes=1000, pretrained=True)
model.eval().cuda()

# Use Compose for transformations
transform = T.Compose([
    T.ToPILImage(),  # Convert numpy array to PIL Image
    T.Resize((256, 128)),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


def extract_reid_embeddings(video_path, tracks):
    cap = cv2.VideoCapture(video_path)
    embeddings = {}
    frame_idx = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret or frame_idx >= len(tracks):
            break
        for track_id, x1, y1, x2, y2 in tracks[frame_idx]:
            crop = frame[int(y1):int(y2), int(x1):int(x2)]
            if crop.size == 0:
                continue
            # Apply the composed transformations
            img = transform(crop).unsqueeze(0).cuda()
            with torch.no_grad():
                feat = model(img).cpu().numpy().flatten()
            if track_id not in embeddings:
                embeddings[track_id] = []
            embeddings[track_id].append(feat)
        frame_idx += 1
    cap.release()
    return {tid: np.mean(feats, axis=0) for tid, feats in embeddings.items()}

import cv2

def compute_homography(src_points, dst_points):
    H, _ = cv2.findHomography(np.array(src_points), np.array(dst_points), cv2.RANSAC)
    return H

def warp_player_position(x, y, H):
    pt = np.array([[[x, y]]], dtype='float32')
    warped = cv2.perspectiveTransform(pt, H)
    return warped[0][0]

from scipy.spatial.distance import cdist

def match_players_nn(embed_tac, embed_brd, threshold=0.5):
    ids_tac = list(embed_tac.keys())
    ids_brd = list(embed_brd.keys())

    tac_feats = np.array([embed_tac[i] for i in ids_tac])
    brd_feats = np.array([embed_brd[j] for j in ids_brd])

    dist_matrix = cdist(tac_feats, brd_feats, metric='cosine')
    mapping = {}

    for i, row in enumerate(dist_matrix):
        j = np.argmin(row)
        reverse_match = np.argmin(dist_matrix[:, j])
        if reverse_match == i and row[j] < threshold:
            mapping[ids_tac[i]] = ids_brd[j]
    return mapping

from scipy.spatial.distance import cdist
from google.colab.patches import cv2_imshow

def compute_homography(src_points, dst_points):
    H, _ = cv2.findHomography(np.array(src_points), np.array(dst_points), cv2.RANSAC)
    return H

def warp_player_position(x, y, H):
    pt = np.array([[[x, y]]], dtype='float32')
    warped = cv2.perspectiveTransform(pt, H)
    return warped[0][0]

def match_players_nn(embed_tac, embed_brd, threshold=0.5):
    """Match track IDs between two cameras based on embedding similarity using Nearest Neighbors."""
    ids_tac = list(embed_tac.keys())
    ids_brd = list(embed_brd.keys())

    tac_feats = np.array([embed_tac[i] for i in ids_tac])
    brd_feats = np.array([embed_brd[j] for j in ids_brd])

    # Calculate cosine distance (lower is more similar)
    dist_matrix = cdist(tac_feats, brd_feats, metric='cosine')
    mapping = {}

    for i, row in enumerate(dist_matrix):
        # Find the best match in broadcast for the current tacticam player
        j = np.argmin(row)
        best_score = 1 - row[j]  # Convert cosine distance to similarity score

        # Check for mutual nearest neighbor and threshold
        # Find the best match in tacticam for the best broadcast match
        reverse_match_idx = np.argmin(dist_matrix[:, j])

        if reverse_match_idx == i and best_score > threshold:
            mapping[ids_tac[i]] = ids_brd[j]

    return mapping


def visualize_matches(tac_path, brd_path, tac_tracks, brd_tracks, mapping, sample_ids):
    tac_cap = cv2.VideoCapture(tac_path)
    brd_cap = cv2.VideoCapture(brd_path)

    for frame_idx in range(min(len(tac_tracks), len(brd_tracks))):
        ret1, tac_frame = tac_cap.read()
        ret2, brd_frame = brd_cap.read()
        if not ret1 or not ret2:
            break

        # Create copies to draw on
        tac_frame_drawn = tac_frame.copy()
        brd_frame_drawn = brd_frame.copy()


        for tid, x1, y1, x2, y2 in tac_tracks[frame_idx]:
            if tid in sample_ids and tid in mapping:
                cv2.rectangle(tac_frame_drawn, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
                cv2.putText(tac_frame_drawn, f'Tac {tid}', (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)

        for bid, x1, y1, x2, y2 in brd_tracks[frame_idx]:
            if bid in mapping.values():
                # Find the corresponding tacticam ID for this broadcast ID
                matching_tac_id = None
                for tac_id, brd_id in mapping.items():
                    if brd_id == bid:
                        matching_tac_id = tac_id
                        break

                if matching_tac_id in sample_ids:
                     cv2.rectangle(brd_frame_drawn, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)
                     cv2.putText(brd_frame_drawn, f'Brd {bid}', (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1)


        combined = cv2.hconcat([tac_frame_drawn, brd_frame_drawn])
        cv2_imshow(combined) # Use cv2_imshow instead of cv2.imshow

        # Note: cv2_imshow does not require cv2.waitKey() or cv2.destroyAllWindows()
        # The frames will be displayed inline in the notebook output.


    tac_cap.release()
    brd_cap.release()

# 1. Inference
tac_dets = run_yolo_inference('/content/drive/MyDrive/tacticam.mp4')
brd_dets = run_yolo_inference('/content/drive/MyDrive/broadcast (1).mp4')

# 2. Tracking
tac_tracks = track_players('/content/drive/MyDrive/tacticam.mp4', tac_dets)
brd_tracks = track_players('/content/drive/MyDrive/broadcast (1).mp4', brd_dets)

# 3. Feature Embedding
tac_embed = extract_reid_embeddings('/content/drive/MyDrive/tacticam.mp4', tac_tracks)
brd_embed = extract_reid_embeddings('/content/drive/MyDrive/broadcast (1).mp4', brd_tracks)

# 4. Matching
player_mapping = match_players_nn(tac_embed, brd_embed)
print("Player Mapping:", player_mapping)

# 5. Visualize
visualize_matches('/content/drive/MyDrive/tacticam.mp4', '/content/drive/MyDrive/broadcast (1).mp4', tac_tracks, brd_tracks, player_mapping, sample_ids=list(player_mapping.keys())[:5])